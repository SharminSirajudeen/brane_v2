# Fly.io Application Configuration for BRANE Backend
# This file defines how your FastAPI application runs on Fly.io infrastructure

# =============================================================================
# APPLICATION IDENTITY
# =============================================================================

# app: Unique identifier for your application on Fly.io platform
# - Must be globally unique across all Fly.io applications
# - Used in the default fly.dev subdomain: brane-backend.fly.dev
# - Change this if the name is already taken
app = "brane-backend"

# primary_region: The main geographic region where your app runs
# - "ord" = Chicago (O'Hare) - good for US central location
# - Other free tier options: "iad" (Virginia), "sjc" (San Jose)
# - Choose based on where most of your users are located
# - Reduces latency and improves response times
primary_region = "ord"

# =============================================================================
# BUILD CONFIGURATION
# =============================================================================

[build]
  # dockerfile: Path to the Dockerfile relative to the project root
  # - Fly.io will use this to build your container image
  # - The Dockerfile contains all build and runtime instructions
  dockerfile = "backend/Dockerfile"

# =============================================================================
# HTTP SERVICE CONFIGURATION
# =============================================================================

[http_service]
  # internal_port: Port your FastAPI app listens on inside the container
  # - Must match the port uvicorn binds to (PORT env var)
  # - Fly.io's proxy forwards external HTTPS traffic to this internal port
  internal_port = 8000

  # force_https: Automatically redirect all HTTP requests to HTTPS
  # - Critical for security: encrypts data in transit
  # - Protects JWT tokens, user credentials, and sensitive data
  # - Set to true for production, even on free tier
  force_https = true

  # auto_stop_machines: Stop VMs when there's no traffic
  # - Saves compute costs by shutting down idle instances
  # - Free tier benefit: only pay for actual usage
  # - Machines restart automatically on next request
  # - Trade-off: first request after idle has ~2-5s cold start
  auto_stop_machines = "stop"

  # auto_start_machines: Automatically start VMs on incoming requests
  # - Works with auto_stop_machines to enable scale-to-zero
  # - First request wakes up the machine
  # - Subsequent requests are fast (machine is warm)
  auto_start_machines = true

  # min_machines_running: Minimum number of VMs to keep running
  # - Set to 0 to enable full scale-to-zero (free tier optimization)
  # - Set to 1+ if you need instant responses (no cold starts)
  # - Trade-off: 0 = save costs, 1+ = better UX
  min_machines_running = 0

  # processes: Which process group(s) this service applies to
  # - ["app"] targets the main application process
  # - Can have multiple process groups (e.g., workers, schedulers)
  processes = ["app"]

  # =============================================================================
  # HEALTH CHECK - LIVENESS PROBE
  # =============================================================================
  # Determines if the application is alive and should receive traffic

  [[http_service.checks]]
    # grace_period: How long to wait before starting health checks after deploy
    # - "30s" gives the app time to start up (init DB, load models, etc.)
    # - Prevents false negatives during startup
    # - Adjust based on your startup time (check logs)
    grace_period = "30s"

    # interval: How often to run the health check
    # - "15s" balances responsiveness vs. overhead
    # - More frequent = faster failure detection, more CPU usage
    # - Less frequent = slower recovery, lower overhead
    interval = "15s"

    # method: HTTP method for health check
    # - "GET" is standard for health endpoints
    # - Should be idempotent (no side effects)
    method = "GET"

    # path: Endpoint to check for health status
    # - "/health" is your FastAPI basic health check endpoint
    # - Returns {"status": "ok", "version": "...", "environment": "..."}
    # - Should respond quickly (< 1s) and not hit the database
    path = "/health"

    # protocol: Protocol to use for health check
    # - "http" checks the internal_port directly
    # - No TLS overhead for internal health checks
    protocol = "http"

    # timeout: Maximum time to wait for a health check response
    # - "10s" is generous for a simple health endpoint
    # - If your endpoint is slower, increase this
    # - If timeout is exceeded, the check fails
    timeout = "10s"

  # =============================================================================
  # CONCURRENCY LIMITS
  # =============================================================================
  # Controls how many concurrent connections/requests a single VM can handle

  [http_service.concurrency]
    # type: Method for measuring concurrency
    # - "requests" = count simultaneous HTTP requests
    # - Alternative: "connections" (less common for HTTP services)
    type = "requests"

    # hard_limit: Maximum concurrent requests before rejecting new ones
    # - 250 is safe for FastAPI on a shared-cpu-1x VM
    # - Protects against overload (OOM, CPU thrashing)
    # - Requests beyond this get 503 Service Unavailable
    # - Tune based on your endpoint's memory/CPU usage
    hard_limit = 250

    # soft_limit: When to trigger scaling up (add more VMs)
    # - 200 gives headroom before hitting hard_limit
    # - Fly.io will try to start additional machines at this point
    # - Only matters if you scale beyond 1 machine
    soft_limit = 200

# =============================================================================
# ENVIRONMENT VARIABLES (Non-sensitive)
# =============================================================================
# These are PUBLIC and visible in logs/config
# NEVER put secrets here - use `fly secrets set` instead

[env]
  # APP_NAME: Application identifier
  # - Used in logs, metrics, and API responses
  APP_NAME = "BRANE"

  # APP_VERSION: Semantic version for tracking deployments
  # - Update when deploying breaking changes
  # - Returned in /health endpoint for debugging
  APP_VERSION = "0.1.0"

  # ENVIRONMENT: Deployment environment identifier
  # - "production" enables production-mode settings
  # - Disables /api/docs and /api/redoc in main.py
  # - Sets appropriate log levels and error handling
  ENVIRONMENT = "production"

  # DEBUG: Debug mode flag
  # - "false" in production for security and performance
  # - "true" enables verbose logging and auto-reload
  # - Never set to "true" in production (security risk)
  DEBUG = "false"

  # HOST: IP address the server binds to
  # - "0.0.0.0" allows connections from any network interface
  # - Required for containerized environments (Docker/Fly.io)
  # - "127.0.0.1" would only accept localhost connections
  HOST = "0.0.0.0"

  # PORT: Port the server listens on
  # - Must match internal_port in [http_service]
  # - Fly.io sets this dynamically, but default is 8000
  # - Uvicorn reads this from the environment
  PORT = "8000"

  # STORAGE_PATH: Directory for file uploads and user data
  # - "/app/storage" is inside the container filesystem
  # - This is ephemeral (lost on VM restart) on free tier
  # - For production, consider using Tigris or S3-compatible storage
  STORAGE_PATH = "/app/storage"

  # AXON_STORAGE_PATH: Directory for agent/axon-specific data
  # - Subdirectory of main storage
  # - Also ephemeral unless using persistent volumes
  AXON_STORAGE_PATH = "/app/storage/axon"

  # MODELS_PATH: Directory for ML models and embeddings
  # - sentence-transformers models (~80MB) are cached here
  # - First request triggers download (slow cold start)
  # - Consider pre-baking models into Docker image for faster starts
  MODELS_PATH = "/app/storage/models"

  # OLLAMA_BASE_URL: URL for local Ollama instance
  # - Not used on Fly.io (Ollama runs separately)
  # - Users can override via neuron configuration
  # - Keep for compatibility with local development
  OLLAMA_BASE_URL = "http://localhost:11434"

  # LOG_LEVEL: Logging verbosity
  # - "INFO" shows important events (startup, errors, requests)
  # - Options: DEBUG (verbose), WARNING (errors only), ERROR (critical only)
  # - "INFO" is a good production default
  LOG_LEVEL = "INFO"

  # LOG_FORMAT: Log output format
  # - "json" enables structured logging for log aggregation
  # - Easier to parse with tools like Fly.io logs, Datadog, etc.
  # - Alternative: "text" for human-readable logs
  LOG_FORMAT = "json"

  # DEFAULT_EMBEDDING_MODEL: Model for text embeddings (RAG)
  # - "sentence-transformers/all-MiniLM-L6-v2" is lightweight (80MB)
  # - Downloads from HuggingFace on first use
  # - Cached in MODELS_PATH for subsequent runs
  # - Used for semantic search in RAG pipeline
  DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

  # DEFAULT_CONTEXT_WINDOW: Max tokens for LLM context
  # - 4096 is a safe default for most models
  # - Override per-neuron for GPT-4 (8k/32k) or Claude (100k)
  DEFAULT_CONTEXT_WINDOW = "4096"

  # DEFAULT_MAX_TOKENS: Max tokens in LLM response
  # - 2048 allows for substantial responses
  # - Lower values reduce cost and latency
  DEFAULT_MAX_TOKENS = "2048"

  # RATE_LIMIT_PER_MINUTE: API rate limiting
  # - 60 requests per minute per user/IP
  # - Protects against abuse and runaway costs
  # - Adjust based on expected usage patterns
  RATE_LIMIT_PER_MINUTE = "60"

# =============================================================================
# SECRETS CONFIGURATION
# =============================================================================
# IMPORTANT: This section documents REQUIRED secrets
# These MUST be set using: fly secrets set KEY=value
#
# NEVER commit actual secret values to version control
# See deployment guide below for the `fly secrets set` commands
#
# Required secrets:
# - DATABASE_URL: PostgreSQL connection string from Neon
# - JWT_SECRET_KEY: Random 32+ character string for JWT signing
# - ENCRYPTION_KEY: Random 32+ character string for data encryption
# - CORS_ORIGINS: Comma-separated list of allowed frontend origins
#
# Optional secrets (if using):
# - GOOGLE_CLIENT_ID: Google OAuth client ID
# - GOOGLE_CLIENT_SECRET: Google OAuth client secret
# - GOOGLE_REDIRECT_URI: OAuth callback URL (https://brane-backend.fly.dev/api/auth/callback)
# - OPENAI_API_KEY: OpenAI API key for GPT models
# - ANTHROPIC_API_KEY: Anthropic API key for Claude models
# - REDIS_URL: Redis connection string (if using caching)
#
# Example commands:
# fly secrets set DATABASE_URL="postgresql://user:pass@host/db"
# fly secrets set JWT_SECRET_KEY="$(openssl rand -hex 32)"
# fly secrets set ENCRYPTION_KEY="$(openssl rand -hex 32)"
# fly secrets set CORS_ORIGINS="https://yourdomain.com,https://yourapp.github.io"
#
# =============================================================================

# =============================================================================
# VM RESOURCES (Compute and Memory)
# =============================================================================

[[vm]]
  # size: VM machine type and resource allocation
  # - "shared-cpu-1x" is the smallest Fly.io VM (free tier eligible)
  # - 1 shared vCPU (burstable, not dedicated)
  # - Good for low-traffic APIs, development, demos
  # - Upgrade to "shared-cpu-2x" or "dedicated-cpu-1x" for production load
  # - See: https://fly.io/docs/about/pricing/#compute
  size = "shared-cpu-1x"

  # memory: RAM allocated to the VM (in MB)
  # - 512MB is the free tier minimum for Python apps
  # - 256MB is too small for FastAPI + sentence-transformers
  # - Increase to 1024MB if you get OOM errors
  # - Memory usage: ~150MB base + ~100MB per model + per-request overhead
  memory = "512mb"

  # processes: Which process group this VM configuration applies to
  # - ["app"] means main FastAPI process
  # - Can have different VM sizes for different process groups
  processes = ["app"]
